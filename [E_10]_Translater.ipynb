{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d906707f",
   "metadata": {},
   "source": [
    "# 프로젝트 : 단어 Level로 번역기 업그레이드하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423cab2f",
   "metadata": {},
   "source": [
    "## 루브릭\n",
    "\n",
    "1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.\t\n",
    " - 구두점, 대소문자, 띄어쓰기 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.\n",
    "2. seq2seq 기반의 번역기 모델이 정상적으로 구동된다.\t\n",
    " - seq2seq 모델 훈련결과를 그래프로 출력해보고, validation loss그래프가 우하향하는 경향성을 보이며 학습이 진행됨이 확인되었다.\n",
    "3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.\t\n",
    " - 테스트용 디코더 모델이 정상적으로 만들어졌으며, input(영어)와 output(프랑스어) 모두 한글로 번역해서 결과를 출력해보았고, 둘의 내용이 유사함을 확인하였다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "238c3626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import pandas as pd\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc26d624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 217975\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>189861</th>\n",
       "      <td>Look both ways before you cross the street.</td>\n",
       "      <td>Regarde de chaque côté avant de traverser la rue.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202549</th>\n",
       "      <td>You don't understand how worried I was about you.</td>\n",
       "      <td>Vous ne comprenez pas combien j'étais soucieux...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14087</th>\n",
       "      <td>Close the hatch.</td>\n",
       "      <td>Ferme la trappe.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132202</th>\n",
       "      <td>That is a well-managed company.</td>\n",
       "      <td>C'est une entreprise bien gérée.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124970</th>\n",
       "      <td>These ties are very expensive.</td>\n",
       "      <td>Ces cravates sont très chères.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "189861        Look both ways before you cross the street.   \n",
       "202549  You don't understand how worried I was about you.   \n",
       "14087                                    Close the hatch.   \n",
       "132202                    That is a well-managed company.   \n",
       "124970                     These ties are very expensive.   \n",
       "\n",
       "                                                      fra  \\\n",
       "189861  Regarde de chaque côté avant de traverser la rue.   \n",
       "202549  Vous ne comprenez pas combien j'étais soucieux...   \n",
       "14087                                    Ferme la trappe.   \n",
       "132202                   C'est une entreprise bien gérée.   \n",
       "124970                     Ces cravates sont très chères.   \n",
       "\n",
       "                                                       cc  \n",
       "189861  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "202549  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "14087   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "132202  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "124970  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07157595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6127</th>\n",
       "      <td>That's weird!</td>\n",
       "      <td>Comme c'est bizarre !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12605</th>\n",
       "      <td>They smell bad.</td>\n",
       "      <td>Elles puent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32128</th>\n",
       "      <td>I can't give it up.</td>\n",
       "      <td>Je ne peux pas y renoncer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4242</th>\n",
       "      <td>We ate eggs.</td>\n",
       "      <td>Nous mangeâmes des œufs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15437</th>\n",
       "      <td>I was born here.</td>\n",
       "      <td>Je suis née ici.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       eng                         fra\n",
       "6127         That's weird!       Comme c'est bizarre !\n",
       "12605      They smell bad.                Elles puent.\n",
       "32128  I can't give it up.  Je ne peux pas y renoncer.\n",
       "4242          We ate eggs.    Nous mangeâmes des œufs.\n",
       "15437     I was born here.            Je suis née ici."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:33000] # 글자 단위가 아닌 단어 단위는 단어장의 크기가 커지고 학습속도가 느려지기에 33000개의 샘플만 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0895a1e6",
   "metadata": {},
   "source": [
    "##  정제, 정규화, 전처리 (영어, 프랑스어 모두)\n",
    "\n",
    "### 1. 구두점(Punctuation)을 단어와 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f25811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20255     I love to travel .\n",
       "18521    You ' ve found it .\n",
       "22673     Tom is in heaven .\n",
       "9480         Untie the dog .\n",
       "27843    She keeps secrets .\n",
       "Name: eng, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'\\w+|[^\\w\\s]'\n",
    "\n",
    "l_eng = lines.eng\n",
    "for i, text in enumerate(l_eng):\n",
    "    \n",
    "    result = re.findall(pattern, text)\n",
    "    l_eng[i] = ' '.join(result)\n",
    "    \n",
    "l_eng.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab861a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3318            Il me faut du temps .\n",
       "20340      J ' ai besoin de liquide .\n",
       "8555                     Sois brève .\n",
       "32983               Je veux y aller .\n",
       "19961    J ' ai rempli la baignoire .\n",
       "Name: fra, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_fra = lines.fra\n",
    "for i, text in enumerate(l_fra):\n",
    "    \n",
    "    result = re.findall(pattern, text)\n",
    "    l_fra[i] = ' '.join(result)\n",
    "    \n",
    "l_fra.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287c69b",
   "metadata": {},
   "source": [
    "- 're'모듈을 사용하여 pattern 정규표현식을 이용하여 구두점과 단어 분리하였다.\n",
    "\n",
    "### 2. 소문자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c3893f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8375          c ' est devenu une épidémie .\n",
       "10076    sommes - nous en train de couler ?\n",
       "9061                      ils ont dit oui .\n",
       "29931                     qui a validé ça ?\n",
       "26274          j ' ai crocheté la serrure .\n",
       "Name: fra, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_eng = l_eng.apply(lambda x: x.lower()) \n",
    "l_fra= l_fra.apply(lambda x: x.lower()) \n",
    "\n",
    "l_eng.sample(5)\n",
    "l_fra.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010ce91f",
   "metadata": {},
   "source": [
    "- lower()를 사용하여 간단하게 소문자로 변환하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20bd14",
   "metadata": {},
   "source": [
    "### 3. 디코더의 문장에 시작 토큰과 종료 토큰 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e742e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = 'sos'\n",
    "eos_token = 'eos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88608b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21344             <sos> continuez à avancer ! <eos>\n",
       "12207              <sos> veuillez patienter . <eos>\n",
       "8488                 <sos> il est trop tard . <eos>\n",
       "14121              <sos> je t ' en félicite ! <eos>\n",
       "4892     <sos> il m ' a laissé m ' en aller . <eos>\n",
       "Name: fra, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_fra = l_fra.apply(lambda x : '<sos> '+ x + ' <eos>')\n",
    "\n",
    "l_fra.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f206b",
   "metadata": {},
   "source": [
    "### 4. 띄어쓰기 단위로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db640e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[25], [25], [25]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=False)\n",
    "eng_tokenizer.fit_on_texts(l_eng)              \n",
    "input_text = eng_tokenizer.texts_to_sequences(l_eng)\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a54ca21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 67, 2], [1, 313, 2], [1, 22, 494, 2]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=False)  \n",
    "fra_tokenizer.fit_on_texts(l_fra)\n",
    "target_text = fra_tokenizer.texts_to_sequences(l_fra)\n",
    "\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b50b1",
   "metadata": {},
   "source": [
    "- Tokenizer(char_level=True)를 하면 문자 단위로 토큰화\n",
    "- Tokenizer(char_level=False)를 하면 단어 단위로 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5303b",
   "metadata": {},
   "source": [
    "## 케라스의 토크나이저로 텍스트를 숫자로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8384767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 4544\n",
      "프랑스어 단어장의 크기 : 8303\n",
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1   # 0번 토큰을 고려하여 +1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67113495",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[char for char in line if char != fra_tokenizer.word_index[eos_token]] for line in target_text] \n",
    "\n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[char for char in line if char != fra_tokenizer.word_index[sos_token]] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d72b8bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 67], [1, 313], [1, 22, 494]]\n",
      "[[67, 2], [313, 2], [22, 494, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98b7b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2497a89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (33000, 8, 4544)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (33000, 17, 8303)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (33000, 17, 8303)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9764006",
   "metadata": {},
   "source": [
    "## 임베딩 층(Embedding layer) 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c75d189",
   "metadata": {},
   "source": [
    "#### 단어의 분산 표현(Distributed Representation)\n",
    "- 모든 단어를 고정 차원의 벡터로 표현할 때, 유사한 맥락에서 나타나는 단어는 그 의미도 비슷하다라는 것인데 이것을 분포 가설(distribution hypothesis)이라고 합니다.\n",
    "\n",
    "- 이런 방식으로 얻어지는 단어 벡터를 단어의 분산 표현(Distributed Representation)이라고 합니다. 위에서 활용했던 방법들과는 달리, 벡터의 특정 차원이 특정 의미를 담고 있는 것이 아니라 의미가 벡터의 여러 차원에 분산되어 있으리라고 여기게 됩니다.\n",
    "\n",
    "- 분산 표현을 사용하면 희소 표현과는 다르게 단어 간의 유사도를 계산으로 구할 수 있다는 장점이 있습니다\n",
    "\n",
    "- Embedding layer는 단어의 분산 표현을 구현하기 위한 레이어입니다. [n x k] 형태의 분산 표현으로 만들 수 있는데 Weight이고 파라미터라고 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b6d45ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4544\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print(eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1c340e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "encoder_inputs = Input(shape=(None, ))\n",
    "encoder_emb = Embedding(eng_vocab_size, 64)(encoder_inputs)\n",
    "encoder_lstm = LSTM(units = 64, return_state = True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_emb)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18a6224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_emb = Embedding(fra_vocab_size, 64)(decoder_inputs) \n",
    "decoder_lstm = LSTM(units = 64, return_sequences = True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_emb, initial_state = encoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cef5fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f70c68c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 64)     290816      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 64)     531392      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 64), (None,  33024       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 64), ( 33024       embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 8303)   539695      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,427,951\n",
      "Trainable params: 1,427,951\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e33782",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train,\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=64, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4a316",
   "metadata": {},
   "source": [
    "계속된 kernel 꺼져서 학습이 불가하였다...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0172e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(15,4))\n",
    "epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "# loss 그래프\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, history.history['loss'], 'r', label = 'train loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'b', label='val loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "# accuracy 그래프\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, history.history['accuracy'], 'r', label='train accuracy')\n",
    "plt.plot(epochs, history.history['val_accuracy'], 'b', label='val accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4614adef",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1911a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_emb = Embedding(fra_vocab_size, 64)(decoder_inputs) \n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_emb, initial_state = decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac7b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b19ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdeef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in [10,200,30,40,501]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50897b58",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "- 시간이 부족하여 로컬 환경에서 했다면 달랐을까 라는 생각도 들었다.\n",
    "- 여러 문장을 정제, 정규화, 전처리을 이용하여 tokenizer에 적용해보았다.\n",
    "- 사양이 좋았다면 많은 데이터 수로 학습해보고 싶다.\n",
    "- Embedding Layer를 추가하여 학습해 보는것을 하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b920e97d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
