{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef795adf",
   "metadata": {},
   "source": [
    "# 프로젝트 : 단어 Level로 번역기 업그레이드하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec0d79",
   "metadata": {},
   "source": [
    "## 루브릭\n",
    "\n",
    "1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.\t\n",
    " - 구두점, 대소문자, 띄어쓰기 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.\n",
    "2. seq2seq 기반의 번역기 모델이 정상적으로 구동된다.\t\n",
    " - seq2seq 모델 훈련결과를 그래프로 출력해보고, validation loss그래프가 우하향하는 경향성을 보이며 학습이 진행됨이 확인되었다.\n",
    "3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.\t\n",
    " - 테스트용 디코더 모델이 정상적으로 만들어졌으며, input(영어)와 output(프랑스어) 모두 한글로 번역해서 결과를 출력해보았고, 둘의 내용이 유사함을 확인하였다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ec97ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import pandas as pd\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd136d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 217975\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59246</th>\n",
       "      <td>What happened exactly?</td>\n",
       "      <td>Que s'est-il produit, exactement ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175323</th>\n",
       "      <td>I can't forget about that stupid movie.</td>\n",
       "      <td>Je n'arrive pas à oublier ce film stupide.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27942</th>\n",
       "      <td>Something's wrong.</td>\n",
       "      <td>Quelque chose ne va pas.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167775</th>\n",
       "      <td>Let's sit down and discuss it calmly.</td>\n",
       "      <td>Asseyons-nous et discutons-en calmement.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65332</th>\n",
       "      <td>That one's for me, too.</td>\n",
       "      <td>Celui-là me revient également.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            eng  \\\n",
       "59246                    What happened exactly?   \n",
       "175323  I can't forget about that stupid movie.   \n",
       "27942                        Something's wrong.   \n",
       "167775    Let's sit down and discuss it calmly.   \n",
       "65332                   That one's for me, too.   \n",
       "\n",
       "                                               fra  \\\n",
       "59246           Que s'est-il produit, exactement ?   \n",
       "175323  Je n'arrive pas à oublier ce film stupide.   \n",
       "27942                     Quelque chose ne va pas.   \n",
       "167775    Asseyons-nous et discutons-en calmement.   \n",
       "65332               Celui-là me revient également.   \n",
       "\n",
       "                                                       cc  \n",
       "59246   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "175323  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "27942   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "167775  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "65332   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43fad4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>I called you.</td>\n",
       "      <td>Je t'ai appelé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18101</th>\n",
       "      <td>Who called them?</td>\n",
       "      <td>Qui les a appelés ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29939</th>\n",
       "      <td>Who believes that?</td>\n",
       "      <td>Qui croit ça ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17655</th>\n",
       "      <td>We all hate Tom.</td>\n",
       "      <td>Nous détestons tous Tom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32669</th>\n",
       "      <td>I love butterflies.</td>\n",
       "      <td>J'adore les papillons.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       eng                       fra\n",
       "4999         I called you.           Je t'ai appelé.\n",
       "18101     Who called them?       Qui les a appelés ?\n",
       "29939   Who believes that?            Qui croit ça ?\n",
       "17655     We all hate Tom.  Nous détestons tous Tom.\n",
       "32669  I love butterflies.    J'adore les papillons."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:33000] # 글자 단위가 아닌 단어 단위는 단어장의 크기가 커지고 학습속도가 느려지기에 33000개의 샘플만 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86e11f",
   "metadata": {},
   "source": [
    "## Step 1. 정제, 정규화, 전처리 (영어, 프랑스어 모두!)\n",
    "\n",
    "### 1. 구두점(Punctuation)을 단어와 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11f5c37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14367       Expect no mercy .\n",
       "20539      I was born there .\n",
       "6521           We have some .\n",
       "31630    He entered my room .\n",
       "6162           They said no .\n",
       "Name: eng, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'\\w+|[^\\w\\s]'\n",
    "\n",
    "l_eng = lines.eng\n",
    "for i, text in enumerate(l_eng):\n",
    "    \n",
    "    result = re.findall(pattern, text)\n",
    "    l_eng[i] = ' '.join(result)\n",
    "    \n",
    "l_eng.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aebf33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26757              Je suis sérieuse .\n",
       "8343              Ça a l ' air bien .\n",
       "31490    Donnez - moi votre couteau .\n",
       "4265              On s ' est perdus .\n",
       "13042       C ' est ce que veut Tom .\n",
       "Name: fra, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_fra = lines.fra\n",
    "for i, text in enumerate(l_fra):\n",
    "    \n",
    "    result = re.findall(pattern, text)\n",
    "    l_fra[i] = ' '.join(result)\n",
    "    \n",
    "l_fra.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a6b997",
   "metadata": {},
   "source": [
    "- 're'모듈을 사용하여 pattern 정규표현식을 이용하여 구두점과 단어 분리하였다.\n",
    "\n",
    "### 2. 소문자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3c7b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28498                      elles haletaient .\n",
       "19183            accordez - moi une seconde !\n",
       "31151    voulez - vous que je vous véhicule ?\n",
       "14032     sont - ils en mesure de nous voir ?\n",
       "1904                           vers le nord .\n",
       "Name: fra, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_eng = l_eng.apply(lambda x: x.lower()) \n",
    "l_fra= l_fra.apply(lambda x: x.lower()) \n",
    "\n",
    "l_eng.sample(5)\n",
    "l_fra.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe0fd8",
   "metadata": {},
   "source": [
    "- lower()를 사용하여 간단하게 소문자로 변환하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded20505",
   "metadata": {},
   "source": [
    "### 3. 디코더의 문장에 시작 토큰과 종료 토큰 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "908a09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = 'sos'\n",
    "eos_token = 'eos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38ec220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20927    <sos> j ' en ai marre du poisson ! <eos>\n",
       "31080           <sos> aimez - vous boston ? <eos>\n",
       "27275          <sos> ce n ' est pas grave . <eos>\n",
       "27935             <sos> quelqu ' un parle . <eos>\n",
       "28118          <sos> c ' est bon pour moi . <eos>\n",
       "Name: fra, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_fra = l_fra.apply(lambda x : '<sos> '+ x + ' <eos>')\n",
    "\n",
    "l_fra.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab93d13",
   "metadata": {},
   "source": [
    "### 4. 띄어쓰기 단위로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5faf3723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[25], [25], [25]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=False)\n",
    "eng_tokenizer.fit_on_texts(l_eng)              \n",
    "input_text = eng_tokenizer.texts_to_sequences(l_eng)\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff9f8976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 67, 2], [1, 313, 2], [1, 22, 494, 2]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=False)  \n",
    "fra_tokenizer.fit_on_texts(l_fra)\n",
    "target_text = fra_tokenizer.texts_to_sequences(l_fra)\n",
    "\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda8a6cc",
   "metadata": {},
   "source": [
    "- Tokenizer(char_level=True)를 하면 문자 단위로 토큰화\n",
    "- Tokenizer(char_level=False)를 하면 단어 단위로 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb85188",
   "metadata": {},
   "source": [
    "## Step 3. 케라스의 토크나이저로 텍스트를 숫자로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "007034b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 4544\n",
      "프랑스어 단어장의 크기 : 8303\n",
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1   # 0번 토큰을 고려하여 +1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56e72b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[char for char in line if char != fra_tokenizer.word_index[eos_token]] for line in target_text] \n",
    "\n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[char for char in line if char != fra_tokenizer.word_index[sos_token]] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "178de6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 67], [1, 313], [1, 22, 494]]\n",
      "[[67, 2], [313, 2], [22, 494, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09fb4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c8e91d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (33000, 8, 4544)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (33000, 17, 8303)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (33000, 17, 8303)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4c6750",
   "metadata": {},
   "source": [
    "## Step 4. 임베딩 층(Embedding layer) 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b9e4fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4544\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print(eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5c40d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "encoder_inputs = Input(shape=(None, ))\n",
    "encoder_emb = Embedding(eng_vocab_size, 64)(encoder_inputs)\n",
    "encoder_lstm = LSTM(units = 64, return_state = True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_emb)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e651d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성.\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_emb = Embedding(fra_vocab_size, 64)(decoder_inputs) \n",
    "decoder_lstm = LSTM(units = 64, return_sequences = True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_emb, initial_state = encoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4db5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a3b9237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 64)     290816      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 64)     531392      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 64), (None,  33024       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 64), ( 33024       embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 8303)   539695      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,427,951\n",
      "Trainable params: 1,427,951\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train,\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=16, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0044b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(15,4))\n",
    "epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "# loss 그래프\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, history.history['loss'], 'r', label = 'train loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'b', label='val loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "# accuracy 그래프\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, history.history['accuracy'], 'r', label='train accuracy')\n",
    "plt.plot(epochs, history.history['val_accuracy'], 'b', label='val accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49131b66",
   "metadata": {},
   "source": [
    "## Step 5. 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_emb = Embedding(fra_vocab_size, 64)(decoder_inputs) \n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_emb, initial_state = decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04884b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbdc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5caf39b",
   "metadata": {},
   "source": [
    "## Step 6. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3730cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in [10,200,30,40,501]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b0289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
